<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Dennis Sobolewski</title>
    <link>/categories/r/</link>
      <atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 08 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>R</title>
      <link>/categories/r/</link>
    </image>
    
    <item>
      <title>Finding COVID-19 Clusters Using R</title>
      <link>/post/spatial-autocorrelation-tests-for-covid-19/</link>
      <pubDate>Fri, 08 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/spatial-autocorrelation-tests-for-covid-19/</guid>
      <description>


&lt;p&gt;COVID-19 data analyses are all the rage at the moment, with COVID datasets being made publicly available at the city, state, and national level. It’s an awesome example of how open data can lead to a better understanding of the world around us. And, the best part is, much of the actual analysis is being done for free! I decided to take a stab it and contribute to the public COVID discorse with my own analysis below.&lt;/p&gt;
&lt;p&gt;Talk about COVID-19 “hot spots” is frequent in the news, often referring to NYC since it has the most COVID-19 cases of any city in the US. Beyond looking at simple COVID case tallies, it is not clear how these hot spots are being determined. How do they determine that one area has a statistically significant higher number of cases than another? Factors such as population and the COVID rates of locations directly next to an area all affect how significance is determined within COVID case data. Enter Moran’s I, a measure of spatial autocorrelation that can be used to test for clustering, or dispersion, of an outcome on a map. I will show you how to perform and interpret a Moran’s I test by applying it to real COVID data for a selection of US cities.&lt;/p&gt;
&lt;div id=&#34;data-sources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Sources&lt;/h2&gt;
&lt;p&gt;For each area you want to test for spatial autocorrelation you will need three main pieces of information.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Total COVID cases&lt;/li&gt;
&lt;li&gt;Estimated population&lt;/li&gt;
&lt;li&gt;Geometry of the area&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;covid-cases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;COVID Cases&lt;/h3&gt;
&lt;p&gt;The most granular datasets I could find for COVID data has cases tallied at the zipcode level for certain cities and states. &lt;a href=&#34;https://dph.illinois.gov/covid19/covid19-statistics&#34;&gt;Here&lt;/a&gt; is an example for the state of Illinois. Unfortunately I did not find a central repository or API that allows you to easily retrieve this data for multiple areas. Instead it appears a city or state will release the data on only their own site, meaning I will need to aggregate the data from multiple sources. For this blog I manually downloaded COVID case data by zipcode for Philadelphia, Chicago, and San Francisco&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;population-and-geometry&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Population and Geometry&lt;/h3&gt;
&lt;p&gt;Finding population and case data was easy thanks to the publicly available Census API and the tidycensus R package. The &lt;a href=&#34;https://www.census.gov/data/developers/guidance/api-user-guide.html&#34;&gt;Census API&lt;/a&gt; is pretty amazing, with thousands of different statistics available at multiple geographic levels. To make things easier, the &lt;a href=&#34;https://github.com/walkerke/tidycensus&#34;&gt;tidycensus&lt;/a&gt; package is a convenient wrapper for this API that makes pulling data a breeze. Tidycensus can automatically pull the geometry of the area boundaries your statistics represent for easy plotting and analysis. Adding the geometry to the returned data converts it to an sf object that can be easily visualized using ggplot2. Here is all the code you need to create a heatmap of household income in the US by county.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_county_income &amp;lt;- get_acs(geography = &amp;quot;county&amp;quot;, variables = &amp;quot;B19013_001&amp;quot;, 
                            shift_geo = TRUE, geometry = TRUE)

ggplot(us_county_income) + 
  geom_sf(aes(fill = estimate), color = NA) + 
  coord_sf(datum = NA) + 
  theme_minimal() + 
  scale_fill_viridis_c()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/income-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For our anaylsis I pulled the total population of each zipcode in the US, with its geometry, and did an inner join with my COVID datasets. I end up with a seperate sf object for Philadelpia, San Francisco, and Chicago.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(phila_covid_sf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Simple feature collection with 6 features and 4 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: 478510.5 ymin: 4419279 xmax: 496536.8 ymax: 4435730
## epsg (SRID):    26918
## proj4string:    +proj=utm +zone=18 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs
## # A tibble: 6 x 5
##   cases zip     pop                                  geometry cases_per_cap
##   &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;dbl&amp;gt;                        &amp;lt;MULTIPOLYGON [m]&amp;gt;         &amp;lt;dbl&amp;gt;
## 1   403 19138 34614 (((485004.9 4433570, 485294.2 4433486, 4…         11.6 
## 2   171 19122 22690 (((486657.7 4426246, 486771.9 4426228, 4…          7.54
## 3   509 19149 59853 (((492066.7 4431838, 492275.7 4432106, 4…          8.50
## 4   627 19124 68905 (((489247.9 4429303, 489557.3 4429251, 4…          9.10
## 5   739 19143 65812 (((478510.5 4422474, 478711.6 4422840, 4…         11.2 
## 6   235 19130 26100 (((483562.3 4425076, 483487.3 4425183, 4…          9.00&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;morans-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Moran’s I&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;../../home/dsobolew/Documents/personal-site/static/data/COVID_spatial/moran_I.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;philadelphia-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Philadelphia Example&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;phila_covid_sf %&amp;gt;%
  ggplot() +
  geom_sf(aes(fill = cases_per_cap), color = NA) +
  coord_sf(datum = NA) +
  theme_minimal() +
  scale_fill_viridis_c(name = &amp;quot;Cases Per 1000&amp;quot;) +
  labs(title = &amp;quot;Philadelphia COVID-19 Cases Per Capita&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/phila_geo-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(spdep)

## Autocorrelation tests for phila
phila_sp &amp;lt;- as(phila_covid_sf, &amp;quot;Spatial&amp;quot;)
phila_nb &amp;lt;- poly2nb(phila_sp, queen = T, row.names = phila_sp$zip)
coords &amp;lt;- coordinates(phila_sp)

plot(phila_sp)
plot(phila_nb, coords = coords, add = T, col = &amp;quot;#F78764&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/phila_queen-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## EBI Morans I
set.seed(1988)
phila_moran_mc &amp;lt;- EBImoran.mc(n = phila_sp$cases, 
                              x = phila_sp$pop, 
                              listw = nb2listw(phila_nb, style = &amp;quot;W&amp;quot;),
                              nsim = 9999)

phila_moran_mc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Monte-Carlo simulation of Empirical Bayes Index (mean subtracted)
## 
## data:  cases: phila_sp$cases, risk population: phila_sp$pop
## weights: nb2listw(phila_nb, style = &amp;quot;W&amp;quot;)
## number of simulations + 1: 10000
## 
## statistic = 0.16173, observed rank = 9711, p-value = 0.0289
## alternative hypothesis: greater&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(phila_moran_mc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/phila_mgl-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;phila_lc_moran &amp;lt;- localmoran(phila_sp$cases_per_cap, 
                             listw = nb2listw(phila_nb, style = &amp;quot;W&amp;quot;), 
                             p.adjust.method = &amp;quot;bonferroni&amp;quot;,
                             alternative = &amp;quot;two.sided&amp;quot;)

phila_lc_moran_tidy &amp;lt;- broom::tidy(phila_lc_moran) %&amp;gt;%
  rename(p_value = 6 ,zip = .rownames, morans_i = 2, z_score = 5) %&amp;gt;%
  select(zip, morans_i, z_score, p_value) %&amp;gt;%
  mutate(morans_i = round(morans_i,3),
         z_score = round(z_score,3),
         p_value = round(p_value,3),
         lag_cases_per_cap = round(lag.listw(var = phila_sp$cases_per_cap, x =  nb2listw(phila_nb, style = &amp;quot;W&amp;quot;)),3),
         lag_mean = round(mean(lag.listw(var = phila_sp$cases_per_cap, x =  nb2listw(phila_nb, style = &amp;quot;W&amp;quot;))),3)
         )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(phila_lc_moran_tidy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   zip   morans_i z_score p_value lag_cases_per_cap lag_mean
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 19127    1.63    2.51    0.037              7.10     10.7
## 2 19137    0.969   1.86    0.254              8.50     10.7
## 3 19147    0.72    2.03    0.296              8.36     10.7
## 4 19123   -0.717  -2.08    0.303              7.88     10.7
## 5 19126    0.838   1.61    0.429             11.6      10.7
## 6 19138    0.282   0.752   1                 15.7      10.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/phila_mlc_plot-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/phila_mlc_plot_sf-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;functional-programming-to-test-many-locations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Functional Programming to Test Many Locations&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/covid_plot-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/covid_plot-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/covid_plot-3.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/morans_gl-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/morans_gl-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/morans_gl-3.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/morans_local-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/morans_local-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;../../post/2020-05-08-spatial-autocorrelation-tests-for-covid-19_files/figure-html/morans_local-3.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##Functional Programming version


sf_plot &amp;lt;- function(data, loc) {

data %&amp;gt;%
  ggplot() +
  geom_sf(aes(fill = cases_per_cap)) +
  scale_fill_gradient(low = &amp;quot;#FFF5F0&amp;quot; , high = &amp;quot;#A50F15&amp;quot;, name = &amp;quot;Cases Per 1000&amp;quot;) +
  labs(title = paste0(loc,&amp;quot; COVID-19 Cases by Zipcode&amp;quot;))  

}

#function to create global moran density plots
global_morans_plot &amp;lt;- function(data, loc){
  
  tibble::enframe(data$res) %&amp;gt;%
    ggplot(aes(x = value)) +
    geom_line(stat = &amp;quot;density&amp;quot;) +
    geom_vline(xintercept = data$statistic, col = &amp;quot;red&amp;quot;) +
    annotate(geom = &amp;quot;text&amp;quot;,x = .25, y = 1.5, label = paste0(&amp;quot;P-Value: &amp;quot;,data$p.value)) +
    labs(title = paste0(&amp;quot;Density Plot of Permutation Outcomes: &amp;quot;,loc),
         subtitle = &amp;quot;Monte-Carlo simulation of Empirical Bayes Index (mean subtracted)&amp;quot;,
         x = &amp;quot;Test Statistic&amp;quot;, 
         y = &amp;quot;Density&amp;quot;)
  
}

#function to create tidy local morans tibble
local_morans_tidy &amp;lt;- function(lm, sp, sf){
  
broom::tidy(lm) %&amp;gt;%
    rename(p_value = 6 ,zip = .rownames, morans_i = 2, z_score = 5) %&amp;gt;%
    inner_join(sf, by = c(&amp;quot;zip&amp;quot;=&amp;quot;zip&amp;quot;)) %&amp;gt;%
    mutate(lag_cases_per_cap = spdep::lag.listw(var = sp$cases_per_cap, x = spdep::nb2listw(spdep::poly2nb(sp,queen = T))),
           lag_mean = mean(lag_cases_per_cap),
           cases_mean = mean(lag_cases_per_cap),
           quad = case_when(
             cases_per_cap &amp;lt; cases_mean &amp;amp; lag_cases_per_cap &amp;lt; lag_mean ~ &amp;quot;Low-Low&amp;quot;,
             cases_per_cap &amp;lt; cases_mean &amp;amp; lag_cases_per_cap &amp;gt;= lag_mean ~ &amp;quot;Low-High&amp;quot;,
             cases_per_cap &amp;gt;= cases_mean &amp;amp; lag_cases_per_cap &amp;lt; lag_mean ~ &amp;quot;High-Low&amp;quot;,
             cases_per_cap &amp;gt;= cases_mean &amp;amp; lag_cases_per_cap &amp;gt;= lag_mean ~ &amp;quot;High-High&amp;quot;
           ))

  
}


## Function to create local morans plots
local_morans_plots &amp;lt;- function(lm_tidied, loc){
  
  ggplot() +
    geom_sf(data = sf::st_as_sf(lm_tidied)) +
    geom_sf(data = sf::st_as_sf(lm_tidied) %&amp;gt;% filter(p_value &amp;lt;= .1), aes(fill = quad)) +
    scale_fill_manual(values = c(&amp;quot;Low-Low&amp;quot;=&amp;quot;#4DAF4A&amp;quot; ,&amp;quot;Low-High&amp;quot;=&amp;quot;#377EB8&amp;quot;,&amp;quot;High-Low&amp;quot;=&amp;quot;#FF7F00&amp;quot;,&amp;quot;High-High&amp;quot;=&amp;quot;#E41A1C&amp;quot;)) +
    labs(title = paste0(loc,&amp;quot; Significant COVID-19 Clustering&amp;quot;), x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;, fill = &amp;quot;&amp;quot;)
  
  
}

#sombine sf objects into a tibble with nested lists
covid_tibble &amp;lt;- tibble(
  location = c(&amp;quot;San Francisco&amp;quot;, &amp;quot;Philadelphia&amp;quot;,&amp;quot;Chicago&amp;quot;),
  covid_sf = list(sf_covid_sf,
                  phila_covid_sf, 
                  chi_covid_sf)
)


morans_results &amp;lt;- covid_tibble %&amp;gt;%
  ##perform global morans I calculation with MC simulations
  mutate(
    covid_map = map2(covid_sf,location,sf_plot),
    covid_sp = map(covid_sf, ~as(., &amp;quot;Spatial&amp;quot;)),    ##create sp object
    global_morans = map(covid_sp, ~ spdep::EBImoran.mc(n = .$cases,
                                                            x = .$pop,
                                                            listw = spdep::nb2listw(spdep::poly2nb(.,queen = T, row.names = .$zip)),
                                                            nsim = 9999)),     ##run global morans I test
    global_morans_tidied = map(global_morans, broom::tidy),    ##Create output plots
    global_moran_plots = map2(global_morans,location,global_morans_plot)) %&amp;gt;%   #perform local morans I calculations
 ##Perform local morans I calculations
  mutate(
    local_morans = map(covid_sp, ~ spdep::localmoran(x = .$cases_per_cap,
                                                          listw = spdep::nb2listw(spdep::poly2nb(.,queen = T, row.names = .$zip)),
                                                          p.adjust.method = &amp;quot;bonferroni&amp;quot;)),    ##run local morans I 
    local_morans_tidied = pmap(list(local_morans, covid_sp, covid_sf), local_morans_tidy),    ##tidy the local morans I output
    local_morans_plots = map2(local_morans_tidied,location,local_morans_plots)    ##Create output plots
         )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exploratory Analysis of Time Series Data with Tidyverts</title>
      <link>/post/exploratory-analysis-of-time-series-data-with-tidyverts/</link>
      <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploratory-analysis-of-time-series-data-with-tidyverts/</guid>
      <description>


&lt;div id=&#34;intro-to-tidyverts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro to Tidyverts&lt;/h2&gt;
&lt;p&gt;Last month, I attended rstudio::conf 2020 and took &lt;a href=&#34;https://robjhyndman.com/&#34;&gt;Rob J Hyndman’s&lt;/a&gt; awesome Tidy Time Series and Forecasting in R workshop. Professor Hyndman highlighted functionality within the &lt;a href=&#34;https://tidyverts.org/&#34;&gt;Tidyverts&lt;/a&gt; packages for exploring and extracting features from time series datasets.Tidyverts is currently comprised of three main packages and works within the Tidyverse framework (i.e. piping and dplyr functions).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tsibble-&lt;/strong&gt; Makes data wrangling and formatting of time series data easier. Formats time series data into a “tsibble” R object so other packages within Tidyverts know how to handle it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feasts-&lt;/strong&gt; Used for extracting features from time series data. Has many useful functions for extracting statistics from a time series that can be used for exploratory analysis, model checking, and comparison.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fable-&lt;/strong&gt; Simplifies the creation for forecasting models for time series data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will review and expand on the exploratory data analysis techniques, primarily using tsibble and feasts, learned at rstudio::conf in this post. The workshop changed my perspective on how time series data can be explored, from a simplistic format with limited options (pretty much just line graphs) to a still growing discipline with room for creative solutions. Tidyverts lets you squeeze more insight from a simple time series than I thought possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-tsibble-object&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating a Tsibble Object&lt;/h2&gt;
&lt;p&gt;To begin using the Tidyvert packages, you first need to convert your data into a tsibble object. I will demonstrate this using a dataset showing quarterly Australian tourism totals by State, Region, and purpose that can be found &lt;a href=&#34;http://robjhyndman.com/data/tourism.xlsx&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(tourism)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   Quarter    Region   State           Purpose  Trips
##   &amp;lt;date&amp;gt;     &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 1998-01-01 Adelaide South Australia Business  135.
## 2 1998-04-01 Adelaide South Australia Business  110.
## 3 1998-07-01 Adelaide South Australia Business  166.
## 4 1998-10-01 Adelaide South Australia Business  127.
## 5 1999-01-01 Adelaide South Australia Business  137.
## 6 1999-04-01 Adelaide South Australia Business  200.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tibble has a Quarter column containing dates that will represent our time variable. Before creating a tsibble you need to make sure the time variable is converted to the correct interval. If you set the index equal to the Quarter variable before transforming it, our tsibble functions will assume the desired interval length is one day with three months of data is missing between observations. The tsibble package makes it easy to change this column from a date to a “qtr” data type using the yearquarter() function. Each variable that uniquely determines a time series we wish to measure should be specified using the “key” parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tourism_ts &amp;lt;- tourism %&amp;gt;%
  mutate(Quarter = tsibble::yearquarter(Quarter)) %&amp;gt;%
  tsibble::as_tsibble(index = Quarter, key = c(Region, State, Purpose)) 

head(tourism_ts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tsibble: 6 x 5 [1Q]
## # Key:       Region, State, Purpose [1]
##   Quarter Region   State           Purpose  Trips
##     &amp;lt;qtr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 1998 Q1 Adelaide South Australia Business  135.
## 2 1998 Q2 Adelaide South Australia Business  110.
## 3 1998 Q3 Adelaide South Australia Business  166.
## 4 1998 Q4 Adelaide South Australia Business  127.
## 5 1999 Q1 Adelaide South Australia Business  137.
## 6 1999 Q2 Adelaide South Australia Business  200.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inspecting the new object displays the key variables and shows the updated data type of the Quarter column. The functions within all of the Tidyverts packages are built to work with tsibble objects based on the index and key specification. Most functions will iterate over each key combination present in the dataset by default. What you would typically use an apply or purrr function to achieve just happens automatically. This makes it simple to visualize and run calculations on many different time series at once. It’s almost disconcerting to see how simple the Tidyverts framework makes time series analysis, especially if you ever worked with creating and working with ts objects in the past. It feels like you’re cheating!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-time-series&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizing Time Series&lt;/h2&gt;
&lt;p&gt;Tsibble objects have their own preset ggplot when using the autoplot() function. Autoplot will create a time series line graph for each unique key combination in our tsibble object by default. The interval being used is always specified at the bottom.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(tourism_ts, Trips) + 
  guides(color = F) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/autoplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since these objects interact nicely with the rest of the tidyverse, dplyr functions can be used to filter, group, and summarise tsibbles to explore the data further. Also to note, autoplot() is just a pre-defined ggplot output based on the data being input and all normal ggplot syntax can be used after to change the look of your plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;holidays_ts &amp;lt;- tourism_ts %&amp;gt;%
  filter(Purpose == &amp;quot;Holiday&amp;quot;) %&amp;gt;%
  group_by(State) %&amp;gt;%
  summarise(Trips = sum(Trips))

holidays_ts %&amp;gt;%
  autoplot(Trips) +
  labs(title = &amp;quot;Australian Holiday Travel&amp;quot;, col = &amp;quot;&amp;quot;) +
  theme(legend.position=&amp;quot;bottom&amp;quot;) +
  guides(col=guide_legend(ncol=4)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/autoplot_dplyr-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Feasts package is full of custom ggplot visualization functions that play nicely with tsibble objects and are super useful for data exploration.&lt;/p&gt;
&lt;div id=&#34;seasonal-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Seasonal Plots&lt;/h3&gt;
&lt;p&gt;The gg_season() function will create a plot that chops our time series data into even periods and plots them on top of each other. This lets us better see trends in our data over a given period. In the below example I took our holiday_ts object, filtered it to the state of Victoria, and created a seasonal plot. It is clear a seasonal trend exists with Q3 typically being the lowest travel quarter when we look at our data from this new perspective.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;holidays_ts %&amp;gt;%
  filter(State == &amp;quot;Victoria&amp;quot;) %&amp;gt;%
  feasts::gg_season(Trips, labels = &amp;quot;right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/gg_season-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we don’t filter the data first, the gg_season() function will automatically facet our plot by every key combination. This lets us visually compare multiple time series when exploring our data. We can see the states with trends similar to Victoria but also those with an opposite trend and high Q3 holiday travel volume.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;holidays_ts %&amp;gt;%
  feasts::gg_season(Trips)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/gg_season_facet-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If the tsibble has a smaller interval, the period being shown on the bottom of this graph can be adjusted. Below I used the tsibbledata::vic_elec dataset that has Victoria household electricity usage down to the half hour. By setting period = “day” in gg_season(), an hourly line is drawn for each day in the dataset allowing us to see typical peak usage times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tsibbledata::vic_elec %&amp;gt;%
  feasts::gg_season(Demand, period = &amp;quot;day&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/gg_season_hourly-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subseries-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subseries Plots&lt;/h3&gt;
&lt;p&gt;Another useful plotting function is feasts::gg_subseries(). This will facet the entire series by a smaller period to allow you to see trends within those subsets. In the below example we can see that holiday trips to Victoria are clearly increasing at a much greater rate in the 1st and 4th quarters compared to the 2nd and 3rd.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;holidays_ts %&amp;gt;%
  filter(State == &amp;quot;Victoria&amp;quot;) %&amp;gt;%
  feasts::gg_subseries(Trips)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/gg_subseries-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once again if you do not filter first the graph is further faceted by the key combinations in the dataset to allow for more detailed visual comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;holidays_ts %&amp;gt;%
  feasts::gg_subseries(Trips)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/gg_subseries_facet-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calendar-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calendar Plots&lt;/h3&gt;
&lt;p&gt;Earo Wang’s &lt;a href=&#34;https://github.com/earowang/sugrrants&#34;&gt;sugrrants&lt;/a&gt; package is not a part of Tidyverts but it helps us create calendar plots that are just too pretty to be left out of this post. Since this is not part of tidyverts it actually is easier to manipulate the data as a tibble. There are two flavors of this plot shown below. The second is particularly useful if you are trying to show more than a few months worth of data in a calendar format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tsibbledata::vic_elec %&amp;gt;%
  as_tibble %&amp;gt;%
  mutate(Hour = lubridate::hour(Time)) %&amp;gt;%
  filter(lubridate::year(Date) == 2013,
         lubridate::month(Date) %in% c(4,5)) %&amp;gt;%
  group_by(Date,Hour) %&amp;gt;%
  summarise(Demand = sum(Demand)) %&amp;gt;%
  mutate(Weekend = if_else(lubridate::wday(Date)  %in% c(1,7), &amp;quot;Weekend&amp;quot;, &amp;quot;Weekday&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = Hour, y = Demand, col = Weekend)) +
  geom_line() +
  sugrrants::facet_calendar(~ Date, ncol = 1) +
  theme_bw() +
  theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/calendar_hourly1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calendar &amp;lt;- tsibbledata::vic_elec %&amp;gt;%
  filter(lubridate::year(Date) == 2013) %&amp;gt;%
  mutate(Hour = lubridate::hour(Time)) %&amp;gt;%
  group_by(Date,Hour) %&amp;gt;%
  summarise(Demand = sum(Demand)) %&amp;gt;%
  mutate(Weekend = if_else(lubridate::wday(Date) %in% c(1,7), &amp;quot;Weekend&amp;quot;, &amp;quot;Weekday&amp;quot;)) %&amp;gt;%
  sugrrants::frame_calendar(
    x = Hour, 
    y = Demand, 
    date = Date, 
    nrow = 4
  ) %&amp;gt;%
  ggplot(aes(x = .Hour, y = .Demand, group = Date, col = Weekend)) +
  geom_line() +
  theme(legend.position = &amp;quot;bottom&amp;quot;)

sugrrants::prettify(calendar, size = 3, label.padding = unit(0.15, &amp;quot;lines&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/calendar_hourly2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acf-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ACF Plots&lt;/h3&gt;
&lt;p&gt;Auto-corrections of lagged values for a time series can provide valuable insight in to seasonal or cyclical trends present in the data. The feasts package a an ACF() function that will provide these auto-correlation values for a tsibble object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;holidays_ts %&amp;gt;%
  filter(State == &amp;quot;Victoria&amp;quot;) %&amp;gt;%
  feasts::ACF(Trips, lag_max = 12) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tsibble: 12 x 3 [1Q]
## # Key:       State [1]
##    State      lag      acf
##    &amp;lt;chr&amp;gt;    &amp;lt;lag&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 Victoria    1Q  0.00755
##  2 Victoria    2Q -0.452  
##  3 Victoria    3Q  0.0374 
##  4 Victoria    4Q  0.828  
##  5 Victoria    5Q -0.0305 
##  6 Victoria    6Q -0.463  
##  7 Victoria    7Q  0.0289 
##  8 Victoria    8Q  0.730  
##  9 Victoria    9Q -0.0735 
## 10 Victoria   10Q -0.442  
## 11 Victoria   11Q -0.00197
## 12 Victoria   12Q  0.660&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Feeding these values into the autoplot() function will create an ACF plot that includes a confidence band to help determine significant autocorrelations. Time series with auto-correlations outside of the confidnce bands at constant intervals indicate seasonality. Creating ACF plots on the residuals left over from a forecast can help determing if there are remaining patterns in the data that your model is not accounting for.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;holidays_ts %&amp;gt;%
  feasts::ACF(Trips) %&amp;gt;%
  autoplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/acf2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-extraction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feature Extraction&lt;/h2&gt;
&lt;p&gt;Tidyverts includes functions that make it easy to extract many different features from a tsibble object. The fabletools::features() function allows you to choose the feature, or set of feastures, that you wish to extract. Feasts contains pre-determined sets of features that can be found with ?fabletools::features_by_pkg(). The below example uses the feasts::feat_stl feature set which results in nine features being extracted that summarise each time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tourism_stl_feats &amp;lt;- tourism_ts %&amp;gt;%
  fabletools::features(Trips, features = feasts::feat_stl) 

head(tourism_stl_feats)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 12
##   Region State Purpose trend_strength seasonal_streng… seasonal_peak_y…
##   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
## 1 Adela… Sout… Busine…          0.451            0.380                3
## 2 Adela… Sout… Holiday          0.541            0.601                1
## 3 Adela… Sout… Other            0.743            0.189                2
## 4 Adela… Sout… Visiti…          0.433            0.446                1
## 5 Adela… Sout… Busine…          0.453            0.140                3
## 6 Adela… Sout… Holiday          0.512            0.244                2
## # … with 6 more variables: seasonal_trough_year &amp;lt;dbl&amp;gt;, spikiness &amp;lt;dbl&amp;gt;,
## #   linearity &amp;lt;dbl&amp;gt;, curvature &amp;lt;dbl&amp;gt;, stl_e_acf1 &amp;lt;dbl&amp;gt;, stl_e_acf10 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is powerful for inspecting your time series in creative ways. The below plot looks at the seasonal and overall trend strength of each time series faceted by state and colored by travel purpose. This lets you see patterns across states not possible with traditional time series plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tourism_stl_feats %&amp;gt;%
  ggplot(aes(x = trend_strength, y = seasonal_strength_year, col = Purpose)) +
  geom_point() +
  facet_wrap(~State)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/stl_feats_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can further use these features to answer specific questions of your time series data. For example, below filters our feature set to the time series with the strongest seasonal pattern. Holiday trips in the snowy mountains of New South Wales have the most seasonality in our dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;most_seasonal &amp;lt;- tourism_stl_feats %&amp;gt;%
  filter(seasonal_strength_year == max(seasonal_strength_year))

tourism_ts %&amp;gt;%
  inner_join(most_seasonal, by = c(&amp;quot;State&amp;quot;, &amp;quot;Region&amp;quot;, &amp;quot;Purpose&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = Quarter, y = Trips)) + geom_line() +
  labs(title = &amp;quot;Most Seasonal Series&amp;quot;) +
  facet_grid(vars(State, Region, Purpose))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/stl_feats_seasonal-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The same can be done for overall trend. Business trips in Western Australia have the strongest positive overall trend in our dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;most_trended &amp;lt;- tourism_ts %&amp;gt;%
  fabletools::features(Trips, feasts::feat_stl) %&amp;gt;%
  filter(trend_strength == max(trend_strength))

tourism_ts %&amp;gt;%
  inner_join(most_trended, by = c(&amp;quot;State&amp;quot;, &amp;quot;Region&amp;quot;, &amp;quot;Purpose&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = Quarter, y = Trips)) + geom_line() +
  labs(title = &amp;quot;Most Trended Series&amp;quot;) +
  facet_grid(vars(State, Region, Purpose))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/stl_trends_seasonal-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;hierarchical-clustering-and-pca-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hierarchical Clustering and PCA Analysis&lt;/h3&gt;
&lt;p&gt;The ability to extract many different features from multiple time series at once opens up many possibilities. These features can be used in PCA or clustering analyses to provide different insights from our data.&lt;/p&gt;
&lt;p&gt;The fabletools::features allows you to pull every feature available in the feasts pacakge by using the fabletools::feature_set function. This will result in 44 features being calculated for each time series. I’m not going to lie- I don’t know what all of these features represent or how they are calculated, but I’m not going to let that stop me from using them for this example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tourism_features &amp;lt;- tourism_ts %&amp;gt;%
  fabletools::features(Trips, fabletools::feature_set(pkgs = &amp;quot;feasts&amp;quot;))

head(tourism_features)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 47
##   Region State Purpose trend_strength seasonal_streng… seasonal_peak_y…
##   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
## 1 Adela… Sout… Busine…          0.451            0.380                3
## 2 Adela… Sout… Holiday          0.541            0.601                1
## 3 Adela… Sout… Other            0.743            0.189                2
## 4 Adela… Sout… Visiti…          0.433            0.446                1
## 5 Adela… Sout… Busine…          0.453            0.140                3
## 6 Adela… Sout… Holiday          0.512            0.244                2
## # … with 41 more variables: seasonal_trough_year &amp;lt;dbl&amp;gt;, spikiness &amp;lt;dbl&amp;gt;,
## #   linearity &amp;lt;dbl&amp;gt;, curvature &amp;lt;dbl&amp;gt;, stl_e_acf1 &amp;lt;dbl&amp;gt;, stl_e_acf10 &amp;lt;dbl&amp;gt;,
## #   acf1 &amp;lt;dbl&amp;gt;, acf10 &amp;lt;dbl&amp;gt;, diff1_acf1 &amp;lt;dbl&amp;gt;, diff1_acf10 &amp;lt;dbl&amp;gt;,
## #   diff2_acf1 &amp;lt;dbl&amp;gt;, diff2_acf10 &amp;lt;dbl&amp;gt;, season_acf1 &amp;lt;dbl&amp;gt;, pacf5 &amp;lt;dbl&amp;gt;,
## #   diff1_pacf5 &amp;lt;dbl&amp;gt;, diff2_pacf5 &amp;lt;dbl&amp;gt;, season_pacf &amp;lt;dbl&amp;gt;,
## #   lambda_guerrero &amp;lt;dbl&amp;gt;, kpss_stat &amp;lt;dbl&amp;gt;, kpss_pvalue &amp;lt;dbl&amp;gt;,
## #   pp_stat &amp;lt;dbl&amp;gt;, pp_pvalue &amp;lt;dbl&amp;gt;, ndiffs &amp;lt;int&amp;gt;, nsdiffs &amp;lt;int&amp;gt;,
## #   bp_stat &amp;lt;dbl&amp;gt;, bp_pvalue &amp;lt;dbl&amp;gt;, lb_stat &amp;lt;dbl&amp;gt;, lb_pvalue &amp;lt;dbl&amp;gt;,
## #   var_tiled_var &amp;lt;dbl&amp;gt;, var_tiled_mean &amp;lt;dbl&amp;gt;, shift_level_max &amp;lt;dbl&amp;gt;,
## #   shift_level_index &amp;lt;dbl&amp;gt;, shift_var_max &amp;lt;dbl&amp;gt;, shift_var_index &amp;lt;dbl&amp;gt;,
## #   shift_kl_max &amp;lt;dbl&amp;gt;, shift_kl_index &amp;lt;dbl&amp;gt;, spectral_entropy &amp;lt;dbl&amp;gt;,
## #   n_crossing_points &amp;lt;int&amp;gt;, n_flat_spots &amp;lt;int&amp;gt;, coef_hurst &amp;lt;dbl&amp;gt;,
## #   stat_arch_lm &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Performing a silhouette analysis reveals an optimal K value of 3 for clustering.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/sillouhette%20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A dendrogram of the resulting clusters shows our three clusters.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/clustering%20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;red-cluster&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Red Cluster&lt;/h3&gt;
&lt;p&gt;The red cluster only contains 2 time series from our original dataset, meaning they were different enough from all other series to necessiate their own cluster. Plotting these reveals they have irregular looking seasonality and a positive trend overall.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/tourism_cluster_3-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/tourism_cluster_3-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/tourism_cluster_3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;red-and-green-clusters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Red and Green Clusters&lt;/h3&gt;
&lt;p&gt;The other two larger clusters contain more time series. Lets look at how they most differ from each other. The below shows the top features with the largest difference between the two clusters. Spikiness is the number one differentiating feature between our 2 larger clusters.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                name        1         2 scaled_abs_diff
## 1         spikiness 31.25642 413.83932      0.17963726
## 2   shift_var_index 41.50370  38.28125      0.16966425
## 3 n_crossing_points 36.94815  39.75000      0.14419970
## 4 shift_level_index 37.95556  49.46875      0.14195205
## 5    shift_kl_index 33.64074  38.78125      0.12730849
## 6         linearity 27.17881  34.06117      0.09652482&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we facet by cluster we can plot the individual time series members for each cluster. When all series are plotted together it is difficult to distinguish their features but if we look at the average trip totals for each cluster the differences become clearer. The red cluster contains our two outliers that we looked at earler and make up fewer trips on average than the other two clusters. The green cluster contains more seasonal series with greater spikiness and little overall trend change. The final blue cluster has less consistent seasonality but a stronger overall positive trend.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/tourism_cluster_all-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/tourism_cluster_all-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pca-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PCA Analysis&lt;/h3&gt;
&lt;p&gt;Principal component analysis on the extracted time series features is another useful method for finding insights. Below is a plot of all time series with the PC1 and PC2 values on each axis. This allows us to see how different specific series are from each other in principal component space. We can see a clear grouping of green dots at the top of plot representing holiday trips. I increased the size of the three series that seem furthest away from the others.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/tourism_features_pca-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we isolate these three outliers we can see that two of them were in our red cluster above. The third series represents holidays in Western Australia with high seasonality and a positive trend. These are the three series that are most unique based on the extracted features and PC analysis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/pca_outliers-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-all-together&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Putting It All Together&lt;/h2&gt;
&lt;p&gt;We can combine our PCA and cluster analysis to see all individual cluster members in PC space. This shows a clear picture of how different clusters are represented by the first two prinicpal components and which members lie on the outskirts of a particular cluster. The few green cluster 2 members that lie within the larger red cluster 1 group are prime candidates for further investigation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/tourism_features_pca_clust-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we facet our PC plot by cluster we can retain the purpose category as the point color. This shows the first cluster comprised of a mix of all travel purposes, the second cluster made primarily of holiday travel, and the two previously identified outliers in the third cluster. The two business series present in cluster 2 are interesting points that could be further investigated.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/2020-02-10-exploratory-analysis-of-time-series-data-with-tidyverts_files/figure-html/tourism_features_pca_clust_facet-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/post/2015-07-23-r-rmarkdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      <guid>/post/2015-07-23-r-rmarkdown/</guid>
      <description>


&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../../post/2015-07-23-r-rmarkdown_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A fancy pie chart.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
