[{"authors":["admin"],"categories":null,"content":"I am a Data Scientist and consultant at Bluetree Network, where I work to help healthcare organizations find insight in their hard-earned data.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Data Scientist and consultant at Bluetree Network, where I work to help healthcare organizations find insight in their hard-earned data.","tags":null,"title":"Dennis Sobolewski","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I'll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I'll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using Academic's Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":["R"],"content":" COVID-19 data analyses are all the rage at the moment, with COVID datasets being made publicly available at the city, state, and national level. It’s an awesome example of how open data can lead to a better understanding of the world around us. And, the best part is, much of the actual analysis is being done for free! I decided to take a stab it and contribute to the public COVID discorse with my own analysis below.\nTalk about COVID-19 “hot spots” is frequent in the news, often referring to NYC since it has the most COVID-19 cases of any city in the US. Beyond looking at simple COVID case tallies, it is not clear how these hot spots are being determined. How do they determine that one area has a statistically significant higher number of cases compared to another? Factors such as population and the COVID rates of locations directly next to an area all affect how significance is determined within COVID case data. Enter Moran’s I, a measure of spatial autocorrelation that can be used to test for clustering, or dispersion, of an outcome on a map. I will show you how to perform and interpret a Moran’s I test by applying it to real COVID data for a selection of US cities.\nData Sources For each area you want to test for spatial autocorrelation you will need three main pieces of information.\nTotal COVID cases Estimated population Geometry of the area  COVID Cases The most granular datasets I could find for COVID data has cases tallied at the zipcode level for certain cities and states. Here is an example for the state of Illinois. Unfortunately I did not find a central repository or API that allows you to easily retrieve this data for multiple areas. Instead it appears a city or state will release the data on only their own site, meaning I will need to aggregate the data from multiple sources. For this blog I manually downloaded COVID case data by zipcode for Philadelphia, Chicago, and San Francisco\n Population and Geometry Finding population and case data was easy thanks to the publicly available Census API and the tidycensus R package. The Census API is pretty amazing, with thousands of different statistics available at multiple geographic levels. To make things easier, the tidycensus package is a convenient wrapper for this API that makes pulling data a breeze. Tidycensus can automatically pull the geometry of the area your statistics represent for plotting and analysis. Adding the geometry to the returned data converts it to an sf object that can be easily visualized using ggplot2. Here is all the code you need to create a heatmap of household income in the US by county.\nus_county_income \u0026lt;- get_acs(geography = \u0026quot;county\u0026quot;, variables = \u0026quot;B19013_001\u0026quot;, shift_geo = TRUE, geometry = TRUE) ggplot(us_county_income) + geom_sf(aes(fill = estimate), color = NA) + coord_sf(datum = NA) + theme_minimal() + scale_fill_viridis_c() For our anaylsis I pulled the total population of each zipcode in the US, with its geometry, and did an inner join with my COVID datasets. I end up with a seperate sf object for Philadelpia, San Francisco, and Chicago.\nhead(phila_covid_sf) ## Simple feature collection with 6 features and 4 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 478510.5 ymin: 4419279 xmax: 496536.8 ymax: 4435730 ## epsg (SRID): 26918 ## proj4string: +proj=utm +zone=18 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs ## # A tibble: 6 x 5 ## cases zip pop geometry cases_per_cap ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;MULTIPOLYGON [m]\u0026gt; \u0026lt;dbl\u0026gt; ## 1 403 19138 34614 (((485004.9 4433570, 485294.2 4433486, 4… 11.6 ## 2 171 19122 22690 (((486657.7 4426246, 486771.9 4426228, 4… 7.54 ## 3 509 19149 59853 (((492066.7 4431838, 492275.7 4432106, 4… 8.50 ## 4 627 19124 68905 (((489247.9 4429303, 489557.3 4429251, 4… 9.10 ## 5 739 19143 65812 (((478510.5 4422474, 478711.6 4422840, 4… 11.2 ## 6 235 19130 26100 (((483562.3 4425076, 483487.3 4425183, 4… 9.00   Moran’s I A Moran’s I test results in a p-value, Moran’s I index, and z-score that can be interpreted as follows:     Moran’s I Output Interpretation    The p-value is not statistically significant You cannot reject the null hypothesis. It is quite possible that the spatial distribution of feature values is the result of random spatial processes. The observed spatial pattern of feature values could very well be one of many, many possible version of complete spatial randomness.  The p-value is statistically significant, and the z-score/Moran’s I index is positive. You may reject the null hypothesis. The spatial distribution of high values and/or low values in the dataset is more spatially clustered than would be expected if underlying spatial processes were random.  The p-value is statistically significant, and the z-score/Moran’s I index is negative You may reject the null hypothesis. The spatial distribution of high values and/or low values in the dataset is more spatially dispersed than would be expected if underlying spatial processes were random. A dispersed spatial pattern often reflects some type of competative process- a feature with a high value repels other features of high values; similarly, a feature with low value repels other features with low values.    Furthermore there is a global and local version of the Moran’s I test. A global Moran’s I test produces one set of test statistics that indicates the level of clustering and dispersion in the data as a whole. A local Moran’s I test will produce a set of test statistics for each area grouping specified in the dataset (in this case zip codes) that tell you whether the measurement feature is statistically higher or lower in that area.\n Philadelphia Example I will start with looking at COVID case numbers in Philadelphia. Combining population and zipcode shapefiles from the Census API with COIVD totals scraped from the city’s data portal allows us to create the following cloropleth.\n\nBased on this graph, would you say there is significant COVID outbreak clusters within Philadelphia? The brightest yellow zipcode certainly looks like a hot spot but is the difference statistically significant? Cloropleth maps such as this are extremely popular, and often showcased by healthcare organizations I work with as examples of their BI capabilities. These visuals look beautiful, but drawing conclusions from them with the naked eye can be difficult or, in some cases, downright deceiving. We can take the guesswork out of interpreting a visual like this with Moran’s I.\nGlobal Moran’s I We will use the spdep package in R to help perform the Moran’s I test. I prefer working with sf objects in R, which is a newer way to store spatial data-structures that adheres to tidy principals, but in order or take advantage of the robust spatial tests available in spdep we must use sp objects instead. To perform these tests I will convert my sf objects to sp, then tidy the results on the back end using the broom package.\nIn order to calculate Moran’s I the spdep function needs to know which zipcodes are close to or far away from each other. Feeding an sp object to the poly2nb() function will build a neighbors list based on regions sharing the same boundary. Below visually shows the relationships this function is producing.\nlibrary(spdep) ## convert philly sf object to sp phila_sp \u0026lt;- as(phila_covid_sf, \u0026quot;Spatial\u0026quot;) ## Create list of neighbors phila_nb \u0026lt;- poly2nb(phila_sp, queen = T, row.names = phila_sp$zip) coords \u0026lt;- coordinates(phila_sp) ##Visualize neighbor relationships plot(phila_sp) plot(phila_nb, coords = coords, add = T, col = \u0026quot;#F78764\u0026quot;) We are now ready to perform the Moran’s I test for spatial autocorrelation. I utilize the EBImoran.mc() function since we are working about COVID rates based on the population of a specific zipcode. This function also automatically performs Monte Carlo simulations for us to compare our observed test statistic against. We are required to add the number of COVID cases and population for a zipcode, as well ad a listw object which a weighted list representing the neighborhood relationships we calculated earlier.\n## EBI Morans I set.seed(1988) phila_moran_mc \u0026lt;- EBImoran.mc(n = phila_sp$cases, x = phila_sp$pop, ## convert neighbors list to a listw object listw = nb2listw(phila_nb, style = \u0026quot;W\u0026quot;), nsim = 9999) The tidied output of our test shows a significant p-value of .0289 and positive Moran’s I test statistic of .162. This indicates that there is significant clustering of COVID cases in Philadelphia based on the COVID data we have. A density plot of the Monte Carlo permutation outcomes futher demonstrates how likely our observed test statistic is.\n## # A tibble: 1 x 5 ## statistic p.value parameter method alternative ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.162 0.0289 9711 Monte-Carlo simulation of Empiri… greater  Local Moran’s I Now that we know there is COVID clustering in Philadelphia, lets run a Local Moran’s I test to further investigate. The local version of this test operates much the same as our previous example, except more work is needed to tidy the output since we are generating a greater number of test statistics. There is no “EBI” version of the Local Moran’s test so I fed the COVID per capita rates directly to the localmoran() function.\nphila_lc_moran \u0026lt;- localmoran(phila_sp$cases_per_cap, ## listw object of neighbors list listw = nb2listw(phila_nb, style = \u0026quot;W\u0026quot;), p.adjust.method = \u0026quot;bonferroni\u0026quot;, alternative = \u0026quot;two.sided\u0026quot;) phila_lc_moran_tidy \u0026lt;- broom::tidy(phila_lc_moran) %\u0026gt;% rename(p_value = 6 ,zip = .rownames, morans_i = 2, z_score = 5) %\u0026gt;% select(zip, morans_i, z_score, p_value) %\u0026gt;% mutate(morans_i = round(morans_i,3), z_score = round(z_score,3), p_value = round(p_value,3), lag_cases_per_cap = round(lag.listw(var = phila_sp$cases_per_cap, x = nb2listw(phila_nb, style = \u0026quot;W\u0026quot;)),3), lag_mean = round(mean(lag.listw(var = phila_sp$cases_per_cap, x = nb2listw(phila_nb, style = \u0026quot;W\u0026quot;))),3) ) First 6 results from our Local Moran’s test\nhead(phila_lc_moran_tidy) ## # A tibble: 6 x 6 ## zip morans_i z_score p_value lag_cases_per_cap lag_mean ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 19127 1.63 2.51 0.037 7.10 10.7 ## 2 19137 0.969 1.86 0.254 8.50 10.7 ## 3 19147 0.72 2.03 0.296 8.36 10.7 ## 4 19123 -0.717 -2.08 0.303 7.88 10.7 ## 5 19126 0.838 1.61 0.429 11.6 10.7 ## 6 19138 0.282 0.752 1 15.7 10.7 There is a built in moran.plot function in spdep to visualize how our zipcodes compare to each other. This plots a zipcode’s COVID cases per capita against the weighted per capita rates of the zipcodes around it. The zipcodes that are highlighted are those that deviate the most from other zipcodes in terms of COVID rates.\nWhen tidying our test output I calculated the spatially lagged cases per capita (aka weighted cases per capita of neighboring zipcodes) for each zipcode and the mean spatially lagged cases per capita of all zipcodes. We can use these values to determine if a zipcode falls within one of the below categories:\n Low-Low - The area has a low number of cases and is surrounded by other areas with low case totals. Low-High - The area has a low number of cases and is surrounded by other areas with high case totals. High-Low - The area has a high number of cases and is surrounded by other areas with low case totals. High-High - The are has a high number of cases and is surrounded by other areas with high case totals.  Here is my visualization staging code that combined the Local Moran’s I results to the Philadelphia sf object and calculates which category a zipcode falls into.\nphila_morans_stage \u0026lt;- phila_covid_sf %\u0026gt;% inner_join(phila_lc_moran_tidy, by = c(\u0026quot;zip\u0026quot;=\u0026quot;zip\u0026quot;)) %\u0026gt;% mutate(quad = case_when( cases_per_cap \u0026lt; lag_mean \u0026amp; lag_cases_per_cap \u0026lt; lag_mean ~ \u0026quot;Low-Low\u0026quot;, cases_per_cap \u0026lt; lag_mean \u0026amp; lag_cases_per_cap \u0026gt;= lag_mean ~ \u0026quot;Low-High\u0026quot;, cases_per_cap \u0026gt;= lag_mean \u0026amp; lag_cases_per_cap \u0026lt; lag_mean ~ \u0026quot;High-Low\u0026quot;, cases_per_cap \u0026gt;= lag_mean \u0026amp; lag_cases_per_cap \u0026gt;= lag_mean ~ \u0026quot;High-High\u0026quot; )) The below graph shows Philadelphia and the zipcodes with a significant result from Local Moran’s I. I used a high p-value to demonstrate how a visualization would look utilizing the different color coded categories we discussed. This allows us to quickly see the zipcodes with significant p-values and how they compare to their neighbors.\n  Functional Programming For Many Locations If you can run an analysis once, then you can create functions and utilize purrr to run it many times. I decided to pull data from 3 different cities, Chicago, Philadelphia, and San Francisco, and run the same analysis above on all of them. My code for automating this process for many different locations can be found at the bottom.\nHere are the popular cloropleth graphs for each location showing the COVID cases per capita of zipcodes. Which city has the most clustering of COVID cases? Let’s find out!\nOur Global Moran’s I test results show all cities have significant clustering of COVID cases. Chicago takes the crown as having the most statistically significant clustering with San Francisco being a close second.\n## # A tibble: 3 x 6 ## city statistic p.value parameter method alternative ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 San Fra… 0.494 0.0005 9995 Monte-Carlo simulation … greater ## 2 Philade… 0.162 0.0297 9703 Monte-Carlo simulation … greater ## 3 Chicago 0.473 0.0001 10000 Monte-Carlo simulation … greater Our Local Moran plots show that San Francisco’s clustering mostly consists of high COVID density areas surrounding each other on the East side of the city. Philadelphia’s only significant clustering is a zipcode with an unusually low number of cases on the West side of the city. Chicago is the tale of two cities, with the lakefront downtown area having a significant clustering of low COVID zipcodes and the West side seeing high clustered zipcodes.\n Caveats and Considerations Moran’s I is a great way to statistically prove clustering or dispersion of a spatial features that the naked eye might not find. However, there are major caveats and considerations when performing such a test. In the COVID case, the number of tests being performed in each zipcode obviously will have an affect on outcomes. In a perfect analysis we would have appropriate sampling from each zipcode to help validate our results. Also, the areas that you wish to compare will affect outcomes. Zipcodes are not standard shapes or sizes that perfectly divide a city based on population. Both of these factors need to be taken into consideration when performing and interpreting a Moran’s I test.\nFuncational Code ##Functional Programming version sf_plot \u0026lt;- function(data, loc) { data %\u0026gt;% ggplot() + geom_sf(aes(fill = cases_per_cap)) + scale_fill_gradient(low = \u0026quot;#FFF5F0\u0026quot; , high = \u0026quot;#A50F15\u0026quot;, name = \u0026quot;Cases Per 1000\u0026quot;) + labs(title = paste0(loc,\u0026quot; COVID-19 Cases by Zipcode\u0026quot;)) } #function to create global moran density plots global_morans_plot \u0026lt;- function(data, loc){ tibble::enframe(data$res) %\u0026gt;% ggplot(aes(x = value)) + geom_line(stat = \u0026quot;density\u0026quot;) + geom_vline(xintercept = data$statistic, col = \u0026quot;red\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;,x = .25, y = 1.5, label = paste0(\u0026quot;P-Value: \u0026quot;,data$p.value)) + labs(title = paste0(\u0026quot;Density Plot of Permutation Outcomes: \u0026quot;,loc), subtitle = \u0026quot;Monte-Carlo simulation of Empirical Bayes Index (mean subtracted)\u0026quot;, x = \u0026quot;Test Statistic\u0026quot;, y = \u0026quot;Density\u0026quot;) } #function to create tidy local morans tibble local_morans_tidy \u0026lt;- function(lm, sp, sf){ broom::tidy(lm) %\u0026gt;% rename(p_value = 6 ,zip = .rownames, morans_i = 2, z_score = 5) %\u0026gt;% inner_join(sf, by = c(\u0026quot;zip\u0026quot;=\u0026quot;zip\u0026quot;)) %\u0026gt;% mutate(lag_cases_per_cap = spdep::lag.listw(var = sp$cases_per_cap, x = spdep::nb2listw(spdep::poly2nb(sp,queen = T))), lag_mean = mean(lag_cases_per_cap), quad = case_when( cases_per_cap \u0026lt; lag_mean \u0026amp; lag_cases_per_cap \u0026lt; lag_mean ~ \u0026quot;Low-Low\u0026quot;, cases_per_cap \u0026lt; lag_mean \u0026amp; lag_cases_per_cap \u0026gt;= lag_mean ~ \u0026quot;Low-High\u0026quot;, cases_per_cap \u0026gt;= lag_mean \u0026amp; lag_cases_per_cap \u0026lt; lag_mean ~ \u0026quot;High-Low\u0026quot;, cases_per_cap \u0026gt;= lag_mean \u0026amp; lag_cases_per_cap \u0026gt;= lag_mean ~ \u0026quot;High-High\u0026quot; )) } ## Function to create local morans plots local_morans_plots \u0026lt;- function(lm_tidied, loc){ ggplot() + geom_sf(data = sf::st_as_sf(lm_tidied)) + geom_sf(data = sf::st_as_sf(lm_tidied) %\u0026gt;% filter(p_value \u0026lt;= .1), aes(fill = quad)) + scale_fill_manual(values = c(\u0026quot;Low-Low\u0026quot;=\u0026quot;#4DAF4A\u0026quot; ,\u0026quot;Low-High\u0026quot;=\u0026quot;#377EB8\u0026quot;,\u0026quot;High-Low\u0026quot;=\u0026quot;#FF7F00\u0026quot;,\u0026quot;High-High\u0026quot;=\u0026quot;#E41A1C\u0026quot;)) + labs(title = paste0(loc,\u0026quot; Significant COVID-19 Clustering\u0026quot;), x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;, fill = \u0026quot;\u0026quot;) } #sombine sf objects into a tibble with nested lists covid_tibble \u0026lt;- tibble( location = c(\u0026quot;San Francisco\u0026quot;, \u0026quot;Philadelphia\u0026quot;,\u0026quot;Chicago\u0026quot;), covid_sf = list(sf_covid_sf, phila_covid_sf, chi_covid_sf) ) morans_results \u0026lt;- covid_tibble %\u0026gt;% ##perform global morans I calculation with MC simulations mutate( covid_map = map2(covid_sf,location,sf_plot), covid_sp = map(covid_sf, ~as(., \u0026quot;Spatial\u0026quot;)), ##create sp object global_morans = map(covid_sp, ~ spdep::EBImoran.mc(n = .$cases, x = .$pop, listw = spdep::nb2listw(spdep::poly2nb(.,queen = T, row.names = .$zip)), nsim = 9999)), ##run global morans I test global_morans_tidied = map(global_morans, broom::tidy), ##Create output plots global_moran_plots = map2(global_morans,location,global_morans_plot)) %\u0026gt;% #perform local morans I calculations ##Perform local morans I calculations mutate( local_morans = map(covid_sp, ~ spdep::localmoran(x = .$cases_per_cap, listw = spdep::nb2listw(spdep::poly2nb(.,queen = T, row.names = .$zip)), p.adjust.method = \u0026quot;bonferroni\u0026quot;)), ##run local morans I local_morans_tidied = pmap(list(local_morans, covid_sp, covid_sf), local_morans_tidy), ##tidy the local morans I output local_morans_plots = map2(local_morans_tidied,location,local_morans_plots) ##Create output plots )   ","date":1588896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588969432,"objectID":"7f291ac2eda1ef9ef3f87134fe1aba8e","permalink":"/post/spatial-autocorrelation-tests-for-covid-19/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/post/spatial-autocorrelation-tests-for-covid-19/","section":"post","summary":"COVID-19 data analyses are all the rage at the moment, with COVID datasets being made publicly available at the city, state, and national level. It’s an awesome example of how open data can lead to a better understanding of the world around us. And, the best part is, much of the actual analysis is being done for free! I decided to take a stab it and contribute to the public COVID discorse with my own analysis below.","tags":["geospatial","sf","autocorrelation"],"title":"Finding COVID-19 Clusters Using R","type":"post"},{"authors":[],"categories":["R"],"content":" Intro to Tidyverts Last month, I attended rstudio::conf 2020 and took Rob J Hyndman’s awesome Tidy Time Series and Forecasting in R workshop. Professor Hyndman highlighted functionality within the Tidyverts packages for exploring and extracting features from time series datasets.Tidyverts is currently comprised of three main packages and works within the Tidyverse framework (i.e. piping and dplyr functions).\n Tsibble- Makes data wrangling and formatting of time series data easier. Formats time series data into a “tsibble” R object so other packages within Tidyverts know how to handle it. Feasts- Used for extracting features from time series data. Has many useful functions for extracting statistics from a time series that can be used for exploratory analysis, model checking, and comparison. Fable- Simplifies the creation for forecasting models for time series data.  I will review and expand on the exploratory data analysis techniques, primarily using tsibble and feasts, learned at rstudio::conf in this post. The workshop changed my perspective on how time series data can be explored, from a simplistic format with limited options (pretty much just line graphs) to a still growing discipline with room for creative solutions. Tidyverts lets you squeeze more insight from a simple time series than I thought possible.\n Creating a Tsibble Object To begin using the Tidyvert packages, you first need to convert your data into a tsibble object. I will demonstrate this using a dataset showing quarterly Australian tourism totals by State, Region, and purpose that can be found here.\nhead(tourism) ## # A tibble: 6 x 5 ## Quarter Region State Purpose Trips ## \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1998-01-01 Adelaide South Australia Business 135. ## 2 1998-04-01 Adelaide South Australia Business 110. ## 3 1998-07-01 Adelaide South Australia Business 166. ## 4 1998-10-01 Adelaide South Australia Business 127. ## 5 1999-01-01 Adelaide South Australia Business 137. ## 6 1999-04-01 Adelaide South Australia Business 200. The tibble has a Quarter column containing dates that will represent our time variable. Before creating a tsibble you need to make sure the time variable is converted to the correct interval. If you set the index equal to the Quarter variable before transforming it, our tsibble functions will assume the desired interval length is one day with three months of data is missing between observations. The tsibble package makes it easy to change this column from a date to a “qtr” data type using the yearquarter() function. Each variable that uniquely determines a time series we wish to measure should be specified using the “key” parameter.\ntourism_ts \u0026lt;- tourism %\u0026gt;% mutate(Quarter = tsibble::yearquarter(Quarter)) %\u0026gt;% tsibble::as_tsibble(index = Quarter, key = c(Region, State, Purpose)) head(tourism_ts) ## # A tsibble: 6 x 5 [1Q] ## # Key: Region, State, Purpose [1] ## Quarter Region State Purpose Trips ## \u0026lt;qtr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1998 Q1 Adelaide South Australia Business 135. ## 2 1998 Q2 Adelaide South Australia Business 110. ## 3 1998 Q3 Adelaide South Australia Business 166. ## 4 1998 Q4 Adelaide South Australia Business 127. ## 5 1999 Q1 Adelaide South Australia Business 137. ## 6 1999 Q2 Adelaide South Australia Business 200. Inspecting the new object displays the key variables and shows the updated data type of the Quarter column. The functions within all of the Tidyverts packages are built to work with tsibble objects based on the index and key specification. Most functions will iterate over each key combination present in the dataset by default. What you would typically use an apply or purrr function to achieve just happens automatically. This makes it simple to visualize and run calculations on many different time series at once. It’s almost disconcerting to see how simple the Tidyverts framework makes time series analysis, especially if you ever worked with creating and working with ts objects in the past. It feels like you’re cheating!\n Visualizing Time Series Tsibble objects have their own preset ggplot when using the autoplot() function. Autoplot will create a time series line graph for each unique key combination in our tsibble object by default. The interval being used is always specified at the bottom.\nautoplot(tourism_ts, Trips) + guides(color = F)  Since these objects interact nicely with the rest of the tidyverse, dplyr functions can be used to filter, group, and summarise tsibbles to explore the data further. Also to note, autoplot() is just a pre-defined ggplot output based on the data being input and all normal ggplot syntax can be used after to change the look of your plots.\nholidays_ts \u0026lt;- tourism_ts %\u0026gt;% filter(Purpose == \u0026quot;Holiday\u0026quot;) %\u0026gt;% group_by(State) %\u0026gt;% summarise(Trips = sum(Trips)) holidays_ts %\u0026gt;% autoplot(Trips) + labs(title = \u0026quot;Australian Holiday Travel\u0026quot;, col = \u0026quot;\u0026quot;) + theme(legend.position=\u0026quot;bottom\u0026quot;) + guides(col=guide_legend(ncol=4))  The Feasts package is full of custom ggplot visualization functions that play nicely with tsibble objects and are super useful for data exploration.\nSeasonal Plots The gg_season() function will create a plot that chops our time series data into even periods and plots them on top of each other. This lets us better see trends in our data over a given period. In the below example I took our holiday_ts object, filtered it to the state of Victoria, and created a seasonal plot. It is clear a seasonal trend exists with Q3 typically being the lowest travel quarter when we look at our data from this new perspective.\nholidays_ts %\u0026gt;% filter(State == \u0026quot;Victoria\u0026quot;) %\u0026gt;% feasts::gg_season(Trips, labels = \u0026quot;right\u0026quot;) If we don’t filter the data first, the gg_season() function will automatically facet our plot by every key combination. This lets us visually compare multiple time series when exploring our data. We can see the states with trends similar to Victoria but also those with an opposite trend and high Q3 holiday travel volume.\nholidays_ts %\u0026gt;% feasts::gg_season(Trips) If the tsibble has a smaller interval, the period being shown on the bottom of this graph can be adjusted. Below I used the tsibbledata::vic_elec dataset that has Victoria household electricity usage down to the half hour. By setting period = “day” in gg_season(), an hourly line is drawn for each day in the dataset allowing us to see typical peak usage times.\ntsibbledata::vic_elec %\u0026gt;% feasts::gg_season(Demand, period = \u0026quot;day\u0026quot;)  Subseries Plots Another useful plotting function is feasts::gg_subseries(). This will facet the entire series by a smaller period to allow you to see trends within those subsets. In the below example we can see that holiday trips to Victoria are clearly increasing at a much greater rate in the 1st and 4th quarters compared to the 2nd and 3rd.\nholidays_ts %\u0026gt;% filter(State == \u0026quot;Victoria\u0026quot;) %\u0026gt;% feasts::gg_subseries(Trips) Once again if you do not filter first the graph is further faceted by the key combinations in the dataset to allow for more detailed visual comparison.\nholidays_ts %\u0026gt;% feasts::gg_subseries(Trips)  Calendar Plots Earo Wang’s sugrrants package is not a part of Tidyverts but it helps us create calendar plots that are just too pretty to be left out of this post. Since this is not part of tidyverts it actually is easier to manipulate the data as a tibble. There are two flavors of this plot shown below. The second is particularly useful if you are trying to show more than a few months worth of data in a calendar format.\ntsibbledata::vic_elec %\u0026gt;% as_tibble %\u0026gt;% mutate(Hour = lubridate::hour(Time)) %\u0026gt;% filter(lubridate::year(Date) == 2013, lubridate::month(Date) %in% c(4,5)) %\u0026gt;% group_by(Date,Hour) %\u0026gt;% summarise(Demand = sum(Demand)) %\u0026gt;% mutate(Weekend = if_else(lubridate::wday(Date) %in% c(1,7), \u0026quot;Weekend\u0026quot;, \u0026quot;Weekday\u0026quot;)) %\u0026gt;% ggplot(aes(x = Hour, y = Demand, col = Weekend)) + geom_line() + sugrrants::facet_calendar(~ Date, ncol = 1) + theme_bw() + theme(legend.position = \u0026quot;bottom\u0026quot;) calendar \u0026lt;- tsibbledata::vic_elec %\u0026gt;% filter(lubridate::year(Date) == 2013) %\u0026gt;% mutate(Hour = lubridate::hour(Time)) %\u0026gt;% group_by(Date,Hour) %\u0026gt;% summarise(Demand = sum(Demand)) %\u0026gt;% mutate(Weekend = if_else(lubridate::wday(Date) %in% c(1,7), \u0026quot;Weekend\u0026quot;, \u0026quot;Weekday\u0026quot;)) %\u0026gt;% sugrrants::frame_calendar( x = Hour, y = Demand, date = Date, nrow = 4 ) %\u0026gt;% ggplot(aes(x = .Hour, y = .Demand, group = Date, col = Weekend)) + geom_line() + theme(legend.position = \u0026quot;bottom\u0026quot;) sugrrants::prettify(calendar, size = 3, label.padding = unit(0.15, \u0026quot;lines\u0026quot;))  ACF Plots Auto-corrections of lagged values for a time series can provide valuable insight in to seasonal or cyclical trends present in the data. The feasts package a an ACF() function that will provide these auto-correlation values for a tsibble object.\nholidays_ts %\u0026gt;% filter(State == \u0026quot;Victoria\u0026quot;) %\u0026gt;% feasts::ACF(Trips, lag_max = 12)  ## # A tsibble: 12 x 3 [1Q] ## # Key: State [1] ## State lag acf ## \u0026lt;chr\u0026gt; \u0026lt;lag\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Victoria 1Q 0.00755 ## 2 Victoria 2Q -0.452 ## 3 Victoria 3Q 0.0374 ## 4 Victoria 4Q 0.828 ## 5 Victoria 5Q -0.0305 ## 6 Victoria 6Q -0.463 ## 7 Victoria 7Q 0.0289 ## 8 Victoria 8Q 0.730 ## 9 Victoria 9Q -0.0735 ## 10 Victoria 10Q -0.442 ## 11 Victoria 11Q -0.00197 ## 12 Victoria 12Q 0.660 Feeding these values into the autoplot() function will create an ACF plot that includes a confidence band to help determine significant autocorrelations. Time series with auto-correlations outside of the confidnce bands at constant intervals indicate seasonality. Creating ACF plots on the residuals left over from a forecast can help determing if there are remaining patterns in the data that your model is not accounting for.\nholidays_ts %\u0026gt;% feasts::ACF(Trips) %\u0026gt;% autoplot()   Feature Extraction Tidyverts includes functions that make it easy to extract many different features from a tsibble object. The fabletools::features() function allows you to choose the feature, or set of feastures, that you wish to extract. Feasts contains pre-determined sets of features that can be found with ?fabletools::features_by_pkg(). The below example uses the feasts::feat_stl feature set which results in nine features being extracted that summarise each time series.\ntourism_stl_feats \u0026lt;- tourism_ts %\u0026gt;% fabletools::features(Trips, features = feasts::feat_stl) head(tourism_stl_feats) ## # A tibble: 6 x 12 ## Region State Purpose trend_strength seasonal_streng… seasonal_peak_y… ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Adela… Sout… Busine… 0.451 0.380 3 ## 2 Adela… Sout… Holiday 0.541 0.601 1 ## 3 Adela… Sout… Other 0.743 0.189 2 ## 4 Adela… Sout… Visiti… 0.433 0.446 1 ## 5 Adela… Sout… Busine… 0.453 0.140 3 ## 6 Adela… Sout… Holiday 0.512 0.244 2 ## # … with 6 more variables: seasonal_trough_year \u0026lt;dbl\u0026gt;, spikiness \u0026lt;dbl\u0026gt;, ## # linearity \u0026lt;dbl\u0026gt;, curvature \u0026lt;dbl\u0026gt;, stl_e_acf1 \u0026lt;dbl\u0026gt;, stl_e_acf10 \u0026lt;dbl\u0026gt; This is powerful for inspecting your time series in creative ways. The below plot looks at the seasonal and overall trend strength of each time series faceted by state and colored by travel purpose. This lets you see patterns across states not possible with traditional time series plots.\ntourism_stl_feats %\u0026gt;% ggplot(aes(x = trend_strength, y = seasonal_strength_year, col = Purpose)) + geom_point() + facet_wrap(~State) You can further use these features to answer specific questions of your time series data. For example, below filters our feature set to the time series with the strongest seasonal pattern. Holiday trips in the snowy mountains of New South Wales have the most seasonality in our dataset.\nmost_seasonal \u0026lt;- tourism_stl_feats %\u0026gt;% filter(seasonal_strength_year == max(seasonal_strength_year)) tourism_ts %\u0026gt;% inner_join(most_seasonal, by = c(\u0026quot;State\u0026quot;, \u0026quot;Region\u0026quot;, \u0026quot;Purpose\u0026quot;)) %\u0026gt;% ggplot(aes(x = Quarter, y = Trips)) + geom_line() + labs(title = \u0026quot;Most Seasonal Series\u0026quot;) + facet_grid(vars(State, Region, Purpose)) The same can be done for overall trend. Business trips in Western Australia have the strongest positive overall trend in our dataset.\nmost_trended \u0026lt;- tourism_ts %\u0026gt;% fabletools::features(Trips, feasts::feat_stl) %\u0026gt;% filter(trend_strength == max(trend_strength)) tourism_ts %\u0026gt;% inner_join(most_trended, by = c(\u0026quot;State\u0026quot;, \u0026quot;Region\u0026quot;, \u0026quot;Purpose\u0026quot;)) %\u0026gt;% ggplot(aes(x = Quarter, y = Trips)) + geom_line() + labs(title = \u0026quot;Most Trended Series\u0026quot;) + facet_grid(vars(State, Region, Purpose)) Hierarchical Clustering and PCA Analysis The ability to extract many different features from multiple time series at once opens up many possibilities. These features can be used in PCA or clustering analyses to provide different insights from our data.\nThe fabletools::features allows you to pull every feature available in the feasts pacakge by using the fabletools::feature_set function. This will result in 44 features being calculated for each time series. I’m not going to lie- I don’t know what all of these features represent or how they are calculated, but I’m not going to let that stop me from using them for this example.\ntourism_features \u0026lt;- tourism_ts %\u0026gt;% fabletools::features(Trips, fabletools::feature_set(pkgs = \u0026quot;feasts\u0026quot;)) head(tourism_features) ## # A tibble: 6 x 47 ## Region State Purpose trend_strength seasonal_streng… seasonal_peak_y… ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Adela… Sout… Busine… 0.451 0.380 3 ## 2 Adela… Sout… Holiday 0.541 0.601 1 ## 3 Adela… Sout… Other 0.743 0.189 2 ## 4 Adela… Sout… Visiti… 0.433 0.446 1 ## 5 Adela… Sout… Busine… 0.453 0.140 3 ## 6 Adela… Sout… Holiday 0.512 0.244 2 ## # … with 41 more variables: seasonal_trough_year \u0026lt;dbl\u0026gt;, spikiness \u0026lt;dbl\u0026gt;, ## # linearity \u0026lt;dbl\u0026gt;, curvature \u0026lt;dbl\u0026gt;, stl_e_acf1 \u0026lt;dbl\u0026gt;, stl_e_acf10 \u0026lt;dbl\u0026gt;, ## # acf1 \u0026lt;dbl\u0026gt;, acf10 \u0026lt;dbl\u0026gt;, diff1_acf1 \u0026lt;dbl\u0026gt;, diff1_acf10 \u0026lt;dbl\u0026gt;, ## # diff2_acf1 \u0026lt;dbl\u0026gt;, diff2_acf10 \u0026lt;dbl\u0026gt;, season_acf1 \u0026lt;dbl\u0026gt;, pacf5 \u0026lt;dbl\u0026gt;, ## # diff1_pacf5 \u0026lt;dbl\u0026gt;, diff2_pacf5 \u0026lt;dbl\u0026gt;, season_pacf \u0026lt;dbl\u0026gt;, ## # lambda_guerrero \u0026lt;dbl\u0026gt;, kpss_stat \u0026lt;dbl\u0026gt;, kpss_pvalue \u0026lt;dbl\u0026gt;, ## # pp_stat \u0026lt;dbl\u0026gt;, pp_pvalue \u0026lt;dbl\u0026gt;, ndiffs \u0026lt;int\u0026gt;, nsdiffs \u0026lt;int\u0026gt;, ## # bp_stat \u0026lt;dbl\u0026gt;, bp_pvalue \u0026lt;dbl\u0026gt;, lb_stat \u0026lt;dbl\u0026gt;, lb_pvalue \u0026lt;dbl\u0026gt;, ## # var_tiled_var \u0026lt;dbl\u0026gt;, var_tiled_mean \u0026lt;dbl\u0026gt;, shift_level_max \u0026lt;dbl\u0026gt;, ## # shift_level_index \u0026lt;dbl\u0026gt;, shift_var_max \u0026lt;dbl\u0026gt;, shift_var_index \u0026lt;dbl\u0026gt;, ## # shift_kl_max \u0026lt;dbl\u0026gt;, shift_kl_index \u0026lt;dbl\u0026gt;, spectral_entropy \u0026lt;dbl\u0026gt;, ## # n_crossing_points \u0026lt;int\u0026gt;, n_flat_spots \u0026lt;int\u0026gt;, coef_hurst \u0026lt;dbl\u0026gt;, ## # stat_arch_lm \u0026lt;dbl\u0026gt; Performing a silhouette analysis reveals an optimal K value of 3 for clustering.\nA dendrogram of the resulting clusters shows our three clusters.\n Red Cluster The red cluster only contains 2 time series from our original dataset, meaning they were different enough from all other series to necessiate their own cluster. Plotting these reveals they have irregular looking seasonality and a positive trend overall.\n Red and Green Clusters The other two larger clusters contain more time series. Lets look at how they most differ from each other. The below shows the top features with the largest difference between the two clusters. Spikiness is the number one differentiating feature between our 2 larger clusters.\n## name 1 2 scaled_abs_diff ## 1 spikiness 31.25642 413.83932 0.17963726 ## 2 shift_var_index 41.50370 38.28125 0.16966425 ## 3 n_crossing_points 36.94815 39.75000 0.14419970 ## 4 shift_level_index 37.95556 49.46875 0.14195205 ## 5 shift_kl_index 33.64074 38.78125 0.12730849 ## 6 linearity 27.17881 34.06117 0.09652482 If we facet by cluster we can plot the individual time series members for each cluster. When all series are plotted together it is difficult to distinguish their features but if we look at the average trip totals for each cluster the differences become clearer. The red cluster contains our two outliers that we looked at earler and make up fewer trips on average than the other two clusters. The green cluster contains more seasonal series with greater spikiness and little overall trend change. The final blue cluster has less consistent seasonality but a stronger overall positive trend.\n PCA Analysis Principal component analysis on the extracted time series features is another useful method for finding insights. Below is a plot of all time series with the PC1 and PC2 values on each axis. This allows us to see how different specific series are from each other in principal component space. We can see a clear grouping of green dots at the top of plot representing holiday trips. I increased the size of the three series that seem furthest away from the others.\nIf we isolate these three outliers we can see that two of them were in our red cluster above. The third series represents holidays in Western Australia with high seasonality and a positive trend. These are the three series that are most unique based on the extracted features and PC analysis.\n  Putting It All Together We can combine our PCA and cluster analysis to see all individual cluster members in PC space. This shows a clear picture of how different clusters are represented by the first two prinicpal components and which members lie on the outskirts of a particular cluster. The few green cluster 2 members that lie within the larger red cluster 1 group are prime candidates for further investigation.\nIf we facet our PC plot by cluster we can retain the purpose category as the point color. This shows the first cluster comprised of a mix of all travel purposes, the second cluster made primarily of holiday travel, and the two previously identified outliers in the third cluster. The two business series present in cluster 2 are interesting points that could be further investigated.\n ","date":1581292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581371311,"objectID":"5099ec74604cfeadd006d5d4e8f16bc3","permalink":"/post/exploratory-analysis-of-time-series-data-with-tidyverts/","publishdate":"2020-02-10T00:00:00Z","relpermalink":"/post/exploratory-analysis-of-time-series-data-with-tidyverts/","section":"post","summary":"Intro to Tidyverts Last month, I attended rstudio::conf 2020 and took Rob J Hyndman’s awesome Tidy Time Series and Forecasting in R workshop. Professor Hyndman highlighted functionality within the Tidyverts packages for exploring and extracting features from time series datasets.Tidyverts is currently comprised of three main packages and works within the Tidyverse framework (i.e. piping and dplyr functions).\n Tsibble- Makes data wrangling and formatting of time series data easier.","tags":[],"title":"Exploratory Analysis of Time Series Data with Tidyverts","type":"post"},{"authors":[],"categories":[],"content":" No R data science blog would be complete without the obligatory introduction tutorial to the awesome Caret (Classification And REgression Training) package written by Max Kuhn. Below is one of the first R markdown writeups that I ever wrote a few years back. It was part exercise in creating a reproducible report and part practice with using caret to build and evaluate models. I dug up my old R project and thought it would make a fitting first post to my personal site.\nSimple Caret Framework Caret is an extremely useful R package that makes training, comparing, and tuning models easier. The goal of this exercise is to demonstrate the simplest implementation of Caret using the Ames Housing Dataset. For this problem we are trying to create a model for predicting the sale price of homes in Ames, Iowa. I remember just starting out and struggling to find answers to basic modeling questions so I will try to address some of those beginner “gotcha” questions that stumped me initially. For the sake of this tutorial I will not touch on feature engineering besides simple pre-processing steps that come built in with Caret.\n The Data Once we remove the unnecessary “Id” column from our training dataset we are left with a mix of 80 numeric and categorical variables. If we take a look we also see there are a large number of missing values in our dataset.\nlibrary(tidyverse) library(caret) train \u0026lt;- read.csv(here::here(\u0026quot;data\u0026quot;,\u0026quot;train.csv\u0026quot;), stringsAsFactors = F) train \u0026lt;- train %\u0026gt;% select(-Id) #Count NAs per column and graph map_df(train, ~sum(is.na(.))) %\u0026gt;% gather() %\u0026gt;% filter(value \u0026gt; 0) %\u0026gt;% ggplot(.,aes(x = reorder(key,-value), y = value)) + geom_bar(stat=\u0026quot;identity\u0026quot;) + labs(title = \u0026quot;Missing Values\u0026quot;,x=\u0026quot;\u0026quot;) + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) Our first decision is what to do with this missing data. After reading through the data documentation it is not entirely clear if the missing values are due to data collection issues or if the absense simply means the value is “none” or 0 in numerical cases. Looking at the plot it appears there are many missing values for the PoolQC variable. It would make sense that many houses would not have a pool and this should in fact be replaced with the category “none”. The missing values for the garage and basement variables all seem to be perfectly correlated making me think if a home did not have a garage they had NAs across the board for those. I am going to assume a missing value should be replaced with “none” if categorical or 0 if numeric. If we believed these were missing due to errors in data collection we would want to impute the missing values with a best estimate. Caret has a few methods for doing this in its preProcess function that are simple to use.\nNote: To replace the NAs for categorical data we will want these variables to be type = character. Once values are replaced we want to then change these to type = factor for modeling\n#Replace missing values with \u0026quot;none\u0026quot; or \u0026quot;0\u0026quot; based on categorical vs numeric train_num \u0026lt;- train %\u0026gt;% select_if(is.numeric) train_cat \u0026lt;- train %\u0026gt;% select_if(is.character) train_num[is.na(train_num)] \u0026lt;- 0 train_cat[is.na(train_cat)] \u0026lt;- \u0026quot;None\u0026quot; train_cat \u0026lt;- map_df(train_cat,as.factor) #Combine replaced dataframes train_pp \u0026lt;- cbind(train_cat,train_num) rm(train_cat,train_num)  Setting our Training Framework Now that we have our data in a clean format we are ready to define our Caret trainControl settings. These settings will determine how a model is evaluated so we can compare apples to apples when looking at results for different models. The most simple evaluation method is to train a model on some portion of the training data, predict the outcome on the portion that was withheld, and then measure the accuracy of the predicted vs actual values. In order to make sure we are not overfitting our model or working with a biased train/test set that could affect results we will use cross validation. In Caret we can use the createFolds function to create an index of stratified samples to be used for cross validation. Here we create 10 folds that we then feed into our trainControl index. Each model will train on 9 of the folds and then predict the SalePrice on the 10th. This will continue until every fold’s outcomes have been predicted based on the other 9 and the results will be averaged. This ensures each model is undergoing the same 10 fold cross validation so we are comparing their results fairly.\n#define training folds and steps for modeling ##set.seed for reproducability when randomly choosing folding set.seed(1108) myFolds \u0026lt;- createFolds(train_pp$SalePrice,10) myControl \u0026lt;- trainControl( verboseIter = F, #prints training progress to console savePredictions = T, index = myFolds )  Lets get Modeling! Now that we have our training method we are ready to start modeling. In the past I remember having a lot of uncertainty at this stage due to the factor data we have in our dataset. Regression models cannot handle factor data so should I create dummy variables prior to training my models? Should these dummy variables also be used for ensemble algorithms like random forest that are able to handle factors? It took me way too long to find out that when using the formula layout (y~.) in Caret it automatically turns all factors into dummy variables and that I was doing extra work for no reason. Also, using the formula method with algos like random forest is fine in most cases. For this write up I experimented with both and found the dummy variable version of random forest actually out performed the factor one.\nThe beauty of Caret is that it recognized algorithms from many different R packages. Once the framework for training and evaluating a model is built it is as easy as changing one variable in most cases to do the same thing on a completely different maching learning algorithm. A list of all models possible for Caret can be found here.\nMultiple Regression We will start with a basic multiple regression model. For regression models it is almost always recommended to center and scale your data beforehand. Caret makes this simple with the preProcess settings in the train function. We will also use “nzv” to remove all varaibles with zero or near zero varianace. It is also also easy to create principal components to be used in our model by adding “pca” to preProcess which we will also explore. For PCA many near zero variance variables might be combined into useful information so we will want to use “zv” to only remove zero variance variables. Below shows the training process and best result for each method. Now we have RMSE values that we can use as a benchmark for comparing other modeling techniques. We can see that the PCA model drastically outperformed the regular multiple regression model.\nlm \u0026lt;- train( SalePrice~., data = train_pp, preProcess = c(\u0026quot;nzv\u0026quot;,\u0026quot;center\u0026quot;,\u0026quot;scale\u0026quot;), metric = \u0026quot;RMSE\u0026quot;, method = \u0026quot;lm\u0026quot;, trControl = myControl ) min(lm$results$RMSE) ## [1] 64894.82 lm_pca \u0026lt;- train( SalePrice~., data = train_pp, preProcess = c(\u0026quot;zv\u0026quot;,\u0026quot;center\u0026quot;,\u0026quot;scale\u0026quot;,\u0026quot;pca\u0026quot;), metric = \u0026quot;RMSE\u0026quot;, method = \u0026quot;lm\u0026quot;, trControl = myControl ) min(lm_pca$results$RMSE) ## [1] 39421.78  GLMNET Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. More information on how this works can be found here. Glmnet is easily interpretable like a multiple regression model but avoids many of the common pitfalls in cases where there are many highly correlated variables. One of the main differences when using glmnet in the Caret framework is that there are tuning parameters. Caret makes it very simple to test many different combinations of tuning parameters by using a tuning grid. If the tuneGrid variable is not specified Caret will choose default values that may yield decent results but can probably be improved upon in most cases. Looking at the Caret methods documentation we can see there are two variables that can be tuned: alpha and lambda. Alpha is always 0-1 while lambda can be infinite. The default settings choose a best lambda of 12563 for us which I will use as a base for choosing values. Glmnet can train many different values of lambda at once so adding a large sequence of these to your tuningGrid will not drastically affect run time. As we can see our optimal glmnet model was an improvement over our other regression models.\nglmnet \u0026lt;- train( SalePrice~., data = train_pp, preProcess = c(\u0026quot;nzv\u0026quot;,\u0026quot;center\u0026quot;,\u0026quot;scale\u0026quot;), metric = \u0026quot;RMSE\u0026quot;, method = \u0026quot;glmnet\u0026quot;, trControl = myControl, tuneGrid = expand.grid( alpha = seq(0,1,.1), lambda = seq(1000,50000,100) ) ) min(glmnet$results$RMSE) ## [1] 37086.6  Random Forest Random Forest models are extremely easy to use with little or no pre-processing necessary. These models are based off of decision tree principals which make them highly adaptable to almost all types of input data. What you gain in ease-of-use you lose in interpretability however and random forest models are typically seen as a “black box” algorithm. When we look at the Caret documentation we can see the only variable that requires training is the mtry varaible. The default settings choose an optimal mtry of 131 so we will pick a range around that number to see if we can improve on this.\nrf \u0026lt;- train( SalePrice~., data = train_pp, metric = \u0026quot;RMSE\u0026quot;, method = \u0026quot;rf\u0026quot;, trControl = myControl, tuneGrid = expand.grid( mtry = seq(70, 150, 5) ) ) min(rf$results$RMSE) ## [1] 36259.78  XGBoost The XGBoost package is another popular modeling tool in R and has been featured in multiple winning submissions for Kaggle online data science competitions. XGBoost creates gradient boosted tree models that can be finely tuned to maximize results. There is also an xgbLiner option for Caret that creates penalized regression models similar to GLMNET. The xgbTree method has many of the same benefits as random forest but requires the tuning of many parameters in order to acheieve the best results. Using the Caret documentation, this beginner’s guide to XGBoost tuning, and good old fashioned trial and error I came up with the below tuning grid as a quick first stab. This is an example where experience is needed to find the best results and modeling can be a bit of an artform. I am certain these results could be improved upon with finer tuning.\nxgbTree \u0026lt;- train( SalePrice~., data = train_pp, metric = \u0026quot;RMSE\u0026quot;, method = \u0026quot;xgbTree\u0026quot;, trControl = myControl, tuneGrid = expand.grid( nrounds= 500, max_depth = c(4,6,8), eta = c(.05,0.1,0.2), gamma = 0, colsample_bytree = c(.5,.7), subsample = c(0.5,0.8), min_child_weight = 0 ) ) min(xgbTree$results$RMSE) ## [1] 34102.92   Comparing Results Caret has an useful resamples function for comparing the results of different models. After combining our models into a list we can use built in plots to compare each model based on the cross validated RMSE results. I find the built in dot plot to be easest to read. Here we are looking for not only the lowest RMSE value but also the least variation among the results for each validation fold. As we can see the XGBoost model performed best with a small variation among results. We can see below that the xgbTree model outperformed all others in these categories based on the below.\n##Compare all models to choose the best one mod_list \u0026lt;- list(lm = lm,pca = lm_pca,glmnet = glmnet,rf = rf, XGBTree = xgbTree) resamp \u0026lt;- resamples(mod_list) dotplot(resamp, metric = \u0026quot;RMSE\u0026quot;)  ","date":1578355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578453055,"objectID":"102ca0855f92ffa105ccf0854c38b23e","permalink":"/post/hello-caret/","publishdate":"2020-01-07T00:00:00Z","relpermalink":"/post/hello-caret/","section":"post","summary":"No R data science blog would be complete without the obligatory introduction tutorial to the awesome Caret (Classification And REgression Training) package written by Max Kuhn. Below is one of the first R markdown writeups that I ever wrote a few years back. It was part exercise in creating a reproducible report and part practice with using caret to build and evaluate models. I dug up my old R project and thought it would make a fitting first post to my personal site.","tags":["Demo"],"title":"Hello Caret","type":"post"},{"authors":["Dennis Sobolewski"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let's make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Dennis Sobolewski","Robert Ford"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932  Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA )  Figure 1: A fancy pie chart.   ","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Dennis Sobolewski","Robert Ford"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]